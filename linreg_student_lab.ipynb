{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression & GLMs — Student Lab\n",
        "\n",
        "Complete all TODOs. Avoid sklearn for core parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic Dataset (with collinearity)\n",
        "We generate data where features can be highly correlated to motivate ridge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n",
            "corr(x0,x1)= 0.9985704465455699\n"
          ]
        }
      ],
      "source": [
        "# Collinearity means that some features are highly correlated with each other.\n",
        "def make_regression(n=400, d=5, noise=0.5, collinear=True):\n",
        "    X = rng.standard_normal((n, d))\n",
        "    if collinear and d >= 2:\n",
        "        X[:, 1] = X[:, 0] * 0.95 + 0.05 * rng.standard_normal(n)\n",
        "    w_true = rng.standard_normal(d)\n",
        "    y = X @ w_true + noise * rng.standard_normal(n)\n",
        "    return X, y, w_true\n",
        "\n",
        "X, y, w_true = make_regression()\n",
        "n, d = X.shape\n",
        "check('shapes', y.shape == (n,))\n",
        "print('corr(x0,x1)=', np.corrcoef(X[:,0], X[:,1])[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — OLS Closed Form\n",
        "\n",
        "### Task 1.1: Closed-form w_hat using solve\n",
        "\n",
        "# TODO: compute w_hat using solve on (X^T X) w = X^T y\n",
        "# HINT: `XtX = X.T@X`, `Xty = X.T@y`, `np.linalg.solve(XtX, Xty)`\n",
        "\n",
        "**Checkpoint:** Why is explicit inverse discouraged?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: w_shape\n"
          ]
        }
      ],
      "source": [
        "# OLS = Ordinary Least Squares means minimizing sum of squared errors\n",
        "# It helps to find out w which minimizes mean squared error between predicted and actual values\n",
        "# TODO\n",
        "XtX = X.T @ X # how much feature overlap with each other \n",
        "Xty = X.T @ y # how strongly each feature correlates with prices\n",
        "w_hat = np.linalg.solve(XtX, Xty)\n",
        "check('w_shape', w_hat.shape == (d,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Evaluate fit + residuals\n",
        "Compute:\n",
        "- predictions y_pred\n",
        "- MSE\n",
        "- residual mean and std\n",
        "\n",
        "**Interview Angle:** What does a structured residual pattern imply (e.g., nonlinearity)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mse 0.22892431689020792 resid_mean 0.02930128510763712 resid_std 0.47756230125633753\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "y_pred = X @ w_hat # predicted values\n",
        "mse = float(np.mean((y_pred - y) ** 2)) # mean squared error\n",
        "resid = y_pred - y   # residuals\n",
        "\n",
        "# if i have low mse = good fit\n",
        "\n",
        "print('mse', mse, 'resid_mean', resid.mean(), 'resid_std', resid.std())\n",
        "check('finite', np.isfinite(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Gradient Descent\n",
        "\n",
        "### Task 2.1: Implement MSE loss + gradient\n",
        "\n",
        "Loss = mean((Xw-y)^2), grad = (2/n) X^T(Xw-y)\n",
        "\n",
        "# TODO: implement `mse_loss_and_grad`\n",
        "\n",
        "**FAANG gotcha:** shapes and constants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: grad_shape\n",
            "OK: finite_loss\n"
          ]
        }
      ],
      "source": [
        "# mean squared error = how wrong my predictions are on average\n",
        "# it's going to penalize large errors more than small errors because of squaring\n",
        "# gradient of mse = it tells us in which direction to change weights to reduce error\n",
        "def mse_loss_and_grad(X, y, w):\n",
        "    # TODO\n",
        "    r = X @ w - y  # residuals = predicted - actual , prediction error\n",
        "    loss = float(np.mean(r * r))  # mean squared error\n",
        "    grad = (2.0 / X.shape[0]) * (X.T @ r)  # gradient of mse means how to change weights to reduce error\n",
        "    return loss, grad\n",
        "\n",
        "w0 = np.zeros(d) # filled with zeros\n",
        "loss0, g0 = mse_loss_and_grad(X, y, w0) \n",
        "check('grad_shape', g0.shape == (d,))\n",
        "check('finite_loss', np.isfinite(loss0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Train with GD + compare to closed-form\n",
        "\n",
        "# TODO: implement a simple GD loop, track loss, and compare final weights to w_hat.\n",
        "\n",
        "**Checkpoint:** How does feature scaling affect GD?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final_loss 0.23136626716013964\n",
            "||w_gd-w_hat|| 1.3769754271702541\n",
            "OK: loss_decreases\n"
          ]
        }
      ],
      "source": [
        "def train_gd(X, y, lr=0.05, steps=500):\n",
        "    # TODO\n",
        "    w = np.zeros(X.shape[1]) # shape[1] = d = number of features(columns)\n",
        "    losses = []\n",
        "    for _ in range(steps):\n",
        "        loss, grad = mse_loss_and_grad(X, y, w)\n",
        "        losses.append(loss)\n",
        "        w = w - lr * grad  # update weights in the direction of negative gradient\n",
        "    return w, losses\n",
        "    \n",
        "\n",
        "w_gd, losses = train_gd(X, y, lr=0.05, steps=500)\n",
        "print('final_loss', losses[-1])\n",
        "print('||w_gd-w_hat||', np.linalg.norm(w_gd - w_hat))\n",
        "check('loss_decreases', losses[-1] <= losses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Ridge Regression (L2)\n",
        "\n",
        "### Task 3.1: Ridge closed-form\n",
        "w = (X^T X + λI)^{-1} X^T y\n",
        "\n",
        "# TODO: implement ridge_solve\n",
        "\n",
        "**Interview Angle:** Why does ridge help under collinearity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ridge Regression = Regularize version of linear regression\n",
        "# It adds a penalty to the loss function\n",
        "def ridge_solve(X, y, lam):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "w_ridge = ridge_solve(X, y, lam=1.0)\n",
        "check('ridge_shape', w_ridge.shape == (d,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Bias/variance demo with train/test split\n",
        "\n",
        "# TODO: split into train/test and compare MSE for multiple lambdas.\n",
        "\n",
        "**Checkpoint:** why can test error improve even when train error worsens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "idx = rng.permutation(n)\n",
        "train = idx[: int(0.7*n)]\n",
        "test = idx[int(0.7*n):]\n",
        "Xtr, ytr = X[train], y[train]\n",
        "Xte, yte = X[test], y[test]\n",
        "\n",
        "lams = [0.0, 0.1, 1.0, 10.0]\n",
        "results = []\n",
        "for lam in lams:\n",
        "    w = ridge_solve(Xtr, ytr, lam=lam) if lam > 0 else np.linalg.solve(Xtr.T@Xtr, Xtr.T@ytr)\n",
        "    tr_mse = np.mean((Xtr@w - ytr)**2)\n",
        "    te_mse = np.mean((Xte@w - yte)**2)\n",
        "    results.append((lam, tr_mse, te_mse))\n",
        "print('lam, train_mse, test_mse')\n",
        "for r in results:\n",
        "    print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — GLM Intuition\n",
        "\n",
        "### Task 4.1: Match tasks to (distribution, link)\n",
        "Fill in a table for:\n",
        "- regression\n",
        "- binary classification\n",
        "- count prediction\n",
        "\n",
        "**Explain:** what changes when you go from OLS to a GLM?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Problem | Target type | Distribution | Link | Loss |\n",
        "|---|---|---|---|---|\n",
        "| House price | continuous | ? | ? | ? |\n",
        "| Fraud | binary | ? | ? | ? |\n",
        "| Clicks per user | count | ? | ? | ? |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Train/test results shown for ridge\n",
        "- Short answers to checkpoint questions\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
